{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "idcma.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbIgToA5oYS/LilXymFmhI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paraggarg37/Assignment/blob/master/idcma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6rRYOvnJfgp",
        "outputId": "f80aefaf-f273-4270-a6cd-c8d3c3a03b15"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/avanigoel/icdm_pqa/master/data/Tools_and_Home_Improvement.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-27 12:48:18--  https://raw.githubusercontent.com/avanigoel/icdm_pqa/master/data/Tools_and_Home_Improvement.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 91790274 (88M) [text/plain]\n",
            "Saving to: ‘Tools_and_Home_Improvement.txt.3’\n",
            "\n",
            "Tools_and_Home_Impr 100%[===================>]  87.54M   152MB/s    in 0.6s    \n",
            "\n",
            "2021-04-27 12:48:22 (152 MB/s) - ‘Tools_and_Home_Improvement.txt.3’ saved [91790274/91790274]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIciBXzEClQY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk5xd78s-T7n"
      },
      "source": [
        "device = \"cuda:0\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYb7-Yl6KKnO",
        "outputId": "88d45ac5-6bf9-47db-e679-ff519348f5d3"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch==1.1.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: torch==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.1.0) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SfUMz9PJ9wC"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7W_pGTkz4ki"
      },
      "source": [
        "class SSTDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, maxlen, is_train=True):\n",
        "       \n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.maxlen = maxlen\n",
        "        self.df = df \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #print(\"calinng get item\")\n",
        "        #print(self.df.loc)\n",
        "        #Selecting the sentence and label at the specified index in the data frame\n",
        "      \n",
        "        question = self.df.iloc[index]['question']\n",
        "        #question = self.df['question'][index]\n",
        "        \n",
        "        answer = self.df.iloc[index]['answer']\n",
        "        #answer = self.df['answer'][index]\n",
        "        \n",
        "        #print(\"calinng get item done\")\n",
        "        label = self.df.iloc[index]['label']\n",
        "        #label = self.df['label'][index] \n",
        "\n",
        "        #print (\"question type \", type(question), type(self.df))\n",
        "\n",
        "        #print('label is {}, question {}, ans {}'.format(label, question, answer))\n",
        "\n",
        "        #print(question, answer, str(answer))\n",
        "        #print(label)\n",
        "\n",
        "        encoded_pair = self.tokenizer.encode_plus(\n",
        "                            question, answer,                      # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                            max_length = self.maxlen, \n",
        "                            padding='max_length',\n",
        "                            truncation=True,         # Truncate all sentences.\n",
        "                            return_token_type_ids=True,\n",
        "                            return_attention_mask = True\n",
        "                          \n",
        "        )\n",
        "\n",
        "        tokens_ids_tensor = torch.tensor(encoded_pair['input_ids'])\n",
        "        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "       \n",
        "        tokens_type_ids_tensor = torch.tensor(encoded_pair['token_type_ids'])\n",
        "\n",
        "        #encoded_pair['token_type_ids'],\n",
        "        return tokens_ids_tensor, attn_mask, tokens_type_ids_tensor, label"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EH-M0Cl-JEm"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, freeze_bert = True):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        #Instantiating BERT model object \n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        #Freeze bert layers\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "        \n",
        "        #Classification layer\n",
        "        self.cls_layer = nn.Linear(768, 1).to(device)\n",
        "\n",
        "    def forward(self, seq, attn_masks, tokens_type_ids):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        #token_type_ids=tokens_type_ids\n",
        "        out = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "\n",
        "        logits = self.cls_layer(out[0][:,0])\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeraL3vJ-RsG"
      },
      "source": [
        "def train(net, criterion, opti, train_loader, val_loader, args):\n",
        "    for ep in range(args.max_eps):\n",
        "        print (\"Running ep : {}\".format(ep))\n",
        "        \n",
        "        for it, (seq, attn_masks, tokens_type_ids, labels) in enumerate(train_loader):\n",
        "            #print (\"Running batch : {}\".format(it))\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq, attn_masks, tokens_type_ids, labels = seq.to(device), attn_masks.to(device), tokens_type_ids.to(device), labels.to(device)\n",
        "\n",
        "            #Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks, tokens_type_ids)\n",
        "\n",
        "            #Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            #Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            #Optimization step\n",
        "            opti.step()\n",
        "\n",
        "            if (it + 1) % args.print_every == 0:\n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} Accuracy : {} complete. Loss : {}\".format(it+1, ep+1, acc, loss.item()))\n",
        "        \n",
        "        val_acc, val_loss = evaluate(net, criterion, val_loader, args)\n",
        "        print(\"Epoch {} complete! Validation Accuracy : {}, Validation Loss : {}\".format(ep, val_acc, val_loss))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCNgjCCO_xF-"
      },
      "source": [
        "def evaluate(net, criterion, dataloader, args):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, tokens_type_ids, labels in dataloader:\n",
        "            seq, attn_masks, tokens_type_ids, labels = seq.to(device), attn_masks.to(device), tokens_type_ids.to(device), labels.to(device)\n",
        "            logits = net(seq, attn_masks, tokens_type_ids)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huuxbRT9_x2L"
      },
      "source": [
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56usAc1nyS2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b9882b-c816-4ceb-f4ac-2044395ca750"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"Tools_and_Home_Improvement.txt\", delimiter = '\\t')\n",
        "qa = df[['question','answer']]\n",
        "from sklearn.utils import shuffle\n",
        "list_of_answers = list(df['answer'])\n",
        "list_of_answers = shuffle(list_of_answers)\n",
        "nqa =  pd.DataFrame({'question': df['question'].tolist(),'answer':list_of_answers})\n",
        "qa['label']=1\n",
        "nqa['label']=0\n",
        "data = pd.DataFrame({'question': qa['question'].tolist() + nqa['question'].tolist(),'answer':qa['answer'].tolist() + nqa['answer'].tolist(), 'label':qa['label'].tolist() + nqa['label'].tolist()})\n",
        "data=shuffle(data)\n",
        "split = int(len(data)*0.9)\n",
        "train_data = data[0:split]\n",
        "test_data = data[split:]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP-u6mCx-sOJ"
      },
      "source": [
        "net = SentimentClassifier(freeze_bert = False).to(device)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "opti = optim.Adam(net.parameters(), lr = 2e-5)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtEwXm2_-1l3"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#Creating instances of training and validation set\n",
        "train_set = SSTDataset(df = train_data, maxlen = 64)\n",
        "val_set = SSTDataset(df = test_data, maxlen = 64)\n",
        "\n",
        "#Creating intsances of training and validation dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size = 64, num_workers = 5)\n",
        "val_loader = DataLoader(val_set, batch_size = 64, num_workers = 5)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9mEBH6y_OnG"
      },
      "source": [
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.max_eps = 50\n",
        "        self.gpu = device\n",
        "        self.print_every = 30\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMppZ4Rz_qAx",
        "outputId": "f2ea082e-2f15-4f4d-b2bf-9f19740590d1"
      },
      "source": [
        "args = Args()\n",
        "train(net, criterion, opti, train_loader, val_loader, args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running ep : 0\n",
            "Iteration 30 of epoch 1 Accuracy : 0.625 complete. Loss : 0.6718505620956421\n",
            "Epoch 0 complete! Validation Accuracy : 0.7515625357627869, Validation Loss : 0.5390904396772385\n",
            "Running ep : 1\n",
            "Iteration 30 of epoch 2 Accuracy : 0.8125 complete. Loss : 0.4632303714752197\n",
            "Epoch 1 complete! Validation Accuracy : 0.7989583015441895, Validation Loss : 0.4681692322095235\n",
            "Running ep : 2\n",
            "Iteration 30 of epoch 3 Accuracy : 0.890625 complete. Loss : 0.24782612919807434\n",
            "Epoch 2 complete! Validation Accuracy : 0.8036458492279053, Validation Loss : 0.5873926083246866\n",
            "Running ep : 3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}